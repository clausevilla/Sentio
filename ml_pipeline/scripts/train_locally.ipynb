{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5Xz4Q3MNX_e"
   },
   "source": [
    "# Sentio Model Training\n",
    "#### (for testing outside application)\n",
    "\n",
    "Training notebook for mental health text classification models.\n",
    "\n",
    "**Available models:**\n",
    "- `logistic_regression` - Fast baseline, good for quick iteration\n",
    "- `random_forest` - Ensemble baseline\n",
    "- `lstm` - Bidirectional LSTM neural network\n",
    "- `transformer` - Custom transformer encoder (no pretrained weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths based on if running locally or in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/mbx/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mbx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mbx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mbx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mbx/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local\n",
      "Data dir: /home/mbx/Documents/projects/gitlab/sentio/data\n",
      "Output dir: /home/mbx/Documents/projects/gitlab/sentio/ml_pipeline/sentio_results/increment\n"
     ]
    }
   ],
   "source": [
    "# Author: Marcus Berggren\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install scikit-learn==1.7.2 swifter\n",
    "    # If you are using Google colab, upload both 'sentio/data/' and 'sentio/ml_pipeline' folders to Colab Notebooks folder on Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Colab Notebooks')\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "from ml_pipeline.preprocessing.preprocessor import DataPreprocessingPipeline\n",
    "preprocessor = DataPreprocessingPipeline()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'ml_pipeline' / 'sentio_results' / 'increment'\n",
    "\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QafbfcRwNguO"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    force=True # Should enforce logs to show\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjGsQHQUNoUn"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "**Change `MODEL_TYPE` to train different models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyJRoL86NmZ1",
    "outputId": "cf13a694-737d-4f95-8c52-34b7560276fd"
   },
   "outputs": [],
   "source": [
    "# MODEL SELECTION\n",
    "# Options: 'logistic_regression', 'random_forest', 'lstm', 'transformer'\n",
    "MODEL_TYPE = 'transformer'\n",
    "\n",
    "# Model-specific configs\n",
    "MODEL_CONFIGS = {\n",
    "    'logistic_regression': {\n",
    "        'max_iter': 1000,\n",
    "        'C': 1.0,\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'tfidf': {\n",
    "            'max_features': 10000,\n",
    "        },\n",
    "    },\n",
    "    'lstm': {\n",
    "        'embed_dim': 128,\n",
    "        'hidden_dim': 128,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'epochs': 7,\n",
    "        'patience':3,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 1e-3,\n",
    "        'max_seq_len': 256,\n",
    "        'vocab_size': 30000,\n",
    "    },\n",
    "    'transformer': {\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.2,\n",
    "    'epochs': 15,\n",
    "    'patience': 5,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-4,\n",
    "    'max_seq_len': 256,\n",
    "    'vocab_size': 30000,\n",
    "}\n",
    "}\n",
    "\n",
    "print(f'Model: {MODEL_TYPE}')\n",
    "print(f'Output: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JwlfFyjNq42"
   },
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsvwQMdT4DYz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure you have the following csv files in 'sentio/data/' folder. In Django application data is taken from db.\n",
    "\n",
    "df_train = pd.read_csv(f'{DATA_DIR}/sentio-data-train.csv')\n",
    "X_train = df_train['text']\n",
    "y_train = df_train['label']\n",
    "\n",
    "df_test = pd.read_csv(f'{DATA_DIR}/sentio-data-test.csv')\n",
    "X_test_fixed = df_test['text']\n",
    "y_test_fixed = df_test['label']\n",
    "\n",
    "df_incremental = pd.read_csv(f'{DATA_DIR}/sentio-data-increment.csv')\n",
    "X_train_incremental = df_incremental['text']\n",
    "y_train_incremental = df_incremental['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hHTw0t-N1Nc",
    "outputId": "353979dc-292e-4964-bf51-d5e3c84e7401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 39,084 samples\n",
      "Train label distribution:\n",
      "  Depression: 11,775 (30.1%)\n",
      "  Normal: 11,957 (30.6%)\n",
      "  Stress: 7,189 (18.4%)\n",
      "  Suicidal: 8,163 (20.9%)\n",
      "\n",
      "Test: 9,844 samples\n",
      "Test label distribution:\n",
      "  Depression: 2,976 (7.6%)\n",
      "  Normal: 2,990 (7.7%)\n",
      "  Stress: 1,834 (4.7%)\n",
      "  Suicidal: 2,044 (5.2%)\n",
      "\n",
      "Incremental: 2,085 samples\n",
      "Incremental label distribution:\n",
      "  Depression: 428 (1.1%)\n",
      "  Normal: 406 (1.0%)\n",
      "  Stress: 824 (2.1%)\n",
      "  Suicidal: 427 (1.1%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f'Train: {len(X_train):,} samples')\n",
    "print('Train label distribution:')\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f'  {label}: {count:,} ({count/len(y_train)*100:.1f}%)')\n",
    "\n",
    "print()\n",
    "print(f'Test: {len(X_test_fixed):,} samples')\n",
    "print('Test label distribution:')\n",
    "unique, counts = np.unique(y_test_fixed, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f'  {label}: {count:,} ({count/len(y_train)*100:.1f}%)')\n",
    "\n",
    "print()\n",
    "print(f'Incremental: {len(X_train_incremental):,} samples')\n",
    "print('Incremental label distribution:')\n",
    "unique, counts = np.unique(y_train_incremental, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f'  {label}: {count:,} ({count/len(y_train)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqmAj6RqN2kE"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLQ3MQgsN377",
    "outputId": "6f8ca036-e882-41b0-a213-e452be29adac"
   },
   "outputs": [],
   "source": [
    "from ml_pipeline.training.trainer import ModelTrainer\n",
    "from ml_pipeline.storage.handler import StorageHandler\n",
    "from datetime import datetime\n",
    "\n",
    "storage = StorageHandler(OUTPUT_DIR)\n",
    "trainer = ModelTrainer(storage)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "job_id = f'{MODEL_TYPE}_{timestamp}'\n",
    "\n",
    "print(f'Job ID: {job_id}')\n",
    "print(f'Device: {trainer.device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA1VGp8uN5bp",
    "outputId": "b6d11267-8475-4546-fbcb-fd8aad763f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config for transformer:\n",
      "  d_model: 256\n",
      "  nhead: 8\n",
      "  num_layers: 4\n",
      "  dim_feedforward: 512\n",
      "  dropout: 0.2\n",
      "  epochs: 15\n",
      "  patience: 5\n",
      "  batch_size: 64\n",
      "  learning_rate: 0.0001\n",
      "  max_seq_len: 256\n",
      "  vocab_size: 30000\n"
     ]
    }
   ],
   "source": [
    "config = MODEL_CONFIGS.get(MODEL_TYPE, {})\n",
    "print(f'Config for {MODEL_TYPE}:')\n",
    "for key, value in config.items():\n",
    "    print(f'  {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbAA9jj0N6b1",
    "outputId": "fb033efd-ff0e-433f-a2c6-1adb9c914a64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 15:54:18,892 - ml_pipeline.training.trainer - INFO - Starting training for transformer\n",
      "2025-12-08 15:54:18,896 - ml_pipeline.training.trainer - INFO - Fitting tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 15:54:20,237 - ml_pipeline.training.trainer - INFO - Vocab size: 30000, Classes: 4\n",
      "2025-12-08 15:54:20,239 - ml_pipeline.training.trainer - INFO - Model parameters: 9,855,236\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "2025-12-08 15:55:56,610 - ml_pipeline.training.trainer - INFO - Epoch 1/15 - Loss: 0.8989, Val Accuracy: 0.7172\n",
      "2025-12-08 15:55:56,657 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 15:57:29,461 - ml_pipeline.training.trainer - INFO - Epoch 2/15 - Loss: 0.5482, Val Accuracy: 0.7765\n",
      "2025-12-08 15:57:29,504 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 15:59:01,927 - ml_pipeline.training.trainer - INFO - Epoch 3/15 - Loss: 0.4434, Val Accuracy: 0.7656\n",
      "2025-12-08 16:00:34,525 - ml_pipeline.training.trainer - INFO - Epoch 4/15 - Loss: 0.3645, Val Accuracy: 0.7841\n",
      "2025-12-08 16:00:34,546 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 16:02:07,477 - ml_pipeline.training.trainer - INFO - Epoch 5/15 - Loss: 0.2974, Val Accuracy: 0.7784\n",
      "2025-12-08 16:03:40,360 - ml_pipeline.training.trainer - INFO - Epoch 6/15 - Loss: 0.2610, Val Accuracy: 0.7727\n",
      "2025-12-08 16:05:13,032 - ml_pipeline.training.trainer - INFO - Epoch 7/15 - Loss: 0.2320, Val Accuracy: 0.7695\n",
      "2025-12-08 16:06:45,635 - ml_pipeline.training.trainer - INFO - Epoch 8/15 - Loss: 0.2165, Val Accuracy: 0.7600\n",
      "2025-12-08 16:08:18,305 - ml_pipeline.training.trainer - INFO - Epoch 9/15 - Loss: 0.2016, Val Accuracy: 0.7624\n",
      "2025-12-08 16:08:18,306 - ml_pipeline.training.trainer - INFO - Early stopping at epoch 9 (no improvement for 5 epochs)\n",
      "2025-12-08 16:08:21,545 - ml_pipeline.storage.handler - INFO - Saved neural model to /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n",
      "2025-12-08 16:08:21,546 - ml_pipeline.training.trainer - INFO - Training complete. Accuracy: 0.7841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 13min 54s, sys: 1.83 s, total: 13min 56s\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'Training {MODEL_TYPE}...')\n",
    "\n",
    "result = trainer.train(\n",
    "    model_name=MODEL_TYPE,\n",
    "    data=(X_train, y_train, X_test_fixed, y_test_fixed),\n",
    "    config=config,\n",
    "    job_id=job_id,\n",
    ")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeYM5QCGN8Nt"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRJ9tbhVN7eP",
    "outputId": "10c82787-9ac5-47fd-913c-765c0ddb0adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: success\n",
      "Model: transformer\n",
      "Path: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n",
      "Metrics:\n",
      "  Accuracy:  0.7841\n",
      "  Precision: 0.7927\n",
      "  Recall:    0.7841\n",
      "  F1 Score:  0.7801\n"
     ]
    }
   ],
   "source": [
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Model: {result['model_type']}\")\n",
    "print(f\"Path: {result['model_path']}\")\n",
    "print(\"Metrics:\")\n",
    "print(f\"  Accuracy:  {result['metrics']['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {result['metrics']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {result['metrics']['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {result['metrics']['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "ciXXzUV7N-n5",
    "outputId": "57ff2ee5-2d16-4d6b-ae44-1a3927dc511e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_20251208_155418_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "cm = np.array(result['metrics']['confusion_matrix'])\n",
    "labels = result['metrics']['confusion_matrix_labels']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title(f'Confusion Matrix - {MODEL_TYPE}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = os.path.join(OUTPUT_DIR, f'{job_id}_confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=150)\n",
    "plt.show()\n",
    "print(f'Saved: {cm_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yj4NTzjjN_8q",
    "outputId": "69360021-3b65-4844-de8e-3a4c184281d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class metrics:\n",
      "Depression           P: 0.800  R: 0.572  F1: 0.667  Support: 2976.0\n",
      "Normal               P: 0.909  R: 0.930  F1: 0.920  Support: 2990.0\n",
      "Stress               P: 0.786  R: 0.917  F1: 0.846  Support: 1834.0\n",
      "Suicidal             P: 0.617  R: 0.761  F1: 0.681  Support: 2044.0\n"
     ]
    }
   ],
   "source": [
    "print('Per-class metrics:')\n",
    "report = result['metrics']['classification_report']\n",
    "for label in labels:\n",
    "    if label in report:\n",
    "        m = report[label]\n",
    "        print(f\"{label:20} P: {m['precision']:.3f}  R: {m['recall']:.3f}  F1: {m['f1-score']:.3f}  Support: {m['support']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZx3U_dUOBFi",
    "outputId": "d60f581c-be2e-4f32-8c40-af233ab07a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_20251208_155418_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_path = os.path.join(OUTPUT_DIR, f'{job_id}_results.json')\n",
    "\n",
    "save_results = {\n",
    "    'job_id': job_id,\n",
    "    'model_type': MODEL_TYPE,\n",
    "    'model_path': result['model_path'],\n",
    "    'config': config,\n",
    "    'data': {\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test_fixed),\n",
    "        'classes': list(set(y_train)),\n",
    "    },\n",
    "    'metrics': result['metrics'],\n",
    "    'timestamp': timestamp,\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(f'Saved: {results_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3St2e4YOCSd"
   },
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoHWQqkaODue",
    "outputId": "8d7561ce-5c2b-4dab-c649-cd36e1b9f3ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:08:27,448 - ml_pipeline.inference.predictor - INFO - Loaded model from /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "======================================================================\n",
      "\n",
      "Text: \"I feel so stressed and anxious about everything lately.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Stress                99.8% <<<\n",
      "      Depression             0.1%\n",
      "      Normal                 0.1%\n",
      "      Suicidal               0.0%\n",
      "\n",
      "Text: \"Life is great, I'm feeling happy and content.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                99.1% <<<\n",
      "      Stress                 0.6%\n",
      "      Depression             0.2%\n",
      "      Suicidal               0.2%\n",
      "\n",
      "Text: \"I don't want to live anymore, everything is hopeless.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Suicidal              92.2% <<<\n",
      "      Depression             7.7%\n",
      "      Normal                 0.1%\n",
      "      Stress                 0.1%\n",
      "\n",
      "Text: \"I can't stop crying and I feel so empty inside.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                60.2% <<<\n",
      "      Depression            28.8%\n",
      "      Stress                 9.4%\n",
      "      Suicidal               1.6%\n",
      "\n",
      "Text: \"I am happy and stressed\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                82.1% <<<\n",
      "      Stress                17.2%\n",
      "      Depression             0.5%\n",
      "      Suicidal               0.2%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from ml_pipeline.inference.predictor import Predictor\n",
    "\n",
    "test_texts = [\n",
    "    \"I feel so stressed and anxious about everything lately.\",\n",
    "    \"Life is great, I'm feeling happy and content.\",\n",
    "    \"I don't want to live anymore, everything is hopeless.\",\n",
    "    \"I can't stop crying and I feel so empty inside.\",\n",
    "    \"I am happy and stressed\"\n",
    "]\n",
    "\n",
    "# Load predictor\n",
    "predictor = Predictor(storage)\n",
    "predictor.load(result['model_path'])\n",
    "\n",
    "print('Sample predictions:')\n",
    "print('=' * 70)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Preprocess text first (same as training)\n",
    "    processed = preprocessor._preprocess_single_text(text)\n",
    "\n",
    "    # Get prediction\n",
    "    pred = predictor.predict(processed)\n",
    "\n",
    "    print(f'\\nText: \"{text}\"')\n",
    "    print('-' * 70)\n",
    "\n",
    "    # Sort probabilities descending\n",
    "    sorted_probs = sorted(pred['probabilities'].items(), key=lambda x: -x[1])\n",
    "\n",
    "    for i, (label, prob) in enumerate(sorted_probs):\n",
    "        if i == 0:\n",
    "            print(f'  >>> {label:20} {prob*100:5.1f}% <<<')\n",
    "        else:\n",
    "            print(f'      {label:20} {prob*100:5.1f}%')\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpzYglL7OFew"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZo9dp8TOGbP",
    "outputId": "777d4a66-a544-4c03-f0eb-0ab30899fb19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Model:      transformer\n",
      "Job ID:     transformer_20251208_155418\n",
      "Accuracy:   0.7841\n",
      "F1 Score:   0.7801\n",
      "Model Path: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n",
      "============================================================\n",
      "All outputs saved to: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment\n"
     ]
    }
   ],
   "source": [
    "base_accuracy = result[\"metrics\"][\"accuracy\"]\n",
    "\n",
    "print('=' * 60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Model:      {MODEL_TYPE}')\n",
    "print(f'Job ID:     {job_id}')\n",
    "print(f'Accuracy:   {base_accuracy:.4f}')\n",
    "print(f'F1 Score:   {result[\"metrics\"][\"f1_score\"]:.4f}')\n",
    "print(f'Model Path: {result[\"model_path\"]}')\n",
    "print('=' * 60)\n",
    "print(f'All outputs saved to: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuE3lLrYDeOH"
   },
   "source": [
    "\n",
    "## Incremental training (fine-tune existing model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOuyQHp_Db7A",
    "outputId": "542f7d61-eded-4ed9-841e-d3e76f5ea089"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:08:31,839 - ml_pipeline.training.trainer - INFO - Starting training for transformer\n",
      "2025-12-08 16:08:31,841 - ml_pipeline.training.trainer - INFO - Loading model from /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from this session: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n",
      "Base model: /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_20251208_155418.pt\n",
      "Training transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:08:32,176 - ml_pipeline.training.trainer - INFO - Added 0 new words\n",
      "2025-12-08 16:08:32,578 - ml_pipeline.training.trainer - INFO - Vocab size: 30000, Classes: 4\n",
      "2025-12-08 16:08:32,586 - ml_pipeline.training.trainer - INFO - Model parameters: 9,855,236\n",
      "2025-12-08 16:08:40,871 - ml_pipeline.training.trainer - INFO - Epoch 1/5 - Loss: 0.5522, Val Accuracy: 0.7928\n",
      "2025-12-08 16:08:40,914 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 16:08:48,677 - ml_pipeline.training.trainer - INFO - Epoch 2/5 - Loss: 0.4801, Val Accuracy: 0.7984\n",
      "2025-12-08 16:08:48,725 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 16:08:56,533 - ml_pipeline.training.trainer - INFO - Epoch 3/5 - Loss: 0.4369, Val Accuracy: 0.7952\n",
      "2025-12-08 16:09:04,809 - ml_pipeline.training.trainer - INFO - Epoch 4/5 - Loss: 0.3945, Val Accuracy: 0.7998\n",
      "2025-12-08 16:09:04,828 - ml_pipeline.training.trainer - INFO -   -> New best model saved!\n",
      "2025-12-08 16:09:12,685 - ml_pipeline.training.trainer - INFO - Epoch 5/5 - Loss: 0.3900, Val Accuracy: 0.7986\n",
      "2025-12-08 16:09:16,669 - ml_pipeline.storage.handler - INFO - Saved neural model to /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_incremental_20251208_155418.pt\n",
      "2025-12-08 16:09:16,671 - ml_pipeline.training.trainer - INFO - Training complete. Accuracy: 0.7998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "CPU times: user 42.1 s, sys: 234 ms, total: 42.3 s\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAINING_MODE = 'incremental'\n",
    "\n",
    "# Option 1: Use model from previous full training in this session\n",
    "# Option 2: Specify a path to any existing model\n",
    "USE_PREVIOUS_RESULT = True  # Set to False to use CUSTOM_MODEL_PATH\n",
    "\n",
    "CUSTOM_MODEL_PATH = str(OUTPUT_DIR / 'transformer_transformer_20251130_003457.pt')  # Your best model\n",
    "\n",
    "if USE_PREVIOUS_RESULT and result:\n",
    "    BASE_MODEL_PATH = result['model_path']\n",
    "    base_accuracy = result['metrics']['accuracy']\n",
    "    print(f\"Using model from this session: {BASE_MODEL_PATH}\")\n",
    "else:\n",
    "    BASE_MODEL_PATH = CUSTOM_MODEL_PATH\n",
    "    # Load base model metrics for comparison\n",
    "    checkpoint = storage.load_neural_model(BASE_MODEL_PATH)\n",
    "    print(f\"Using custom model: {BASE_MODEL_PATH}\")\n",
    "    print(\"Note: Run evaluation on base model first to get base_accuracy for comparison\")\n",
    "    base_accuracy = None  # Will need to evaluate separately\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL_PATH}\")\n",
    "\n",
    "# Config for incremental\n",
    "incremental_config = {\n",
    "    'training_mode': 'incremental',\n",
    "    'base_model_path': BASE_MODEL_PATH,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'patience': 3,\n",
    "    'batch_size': 32,\n",
    "    'max_seq_len': 256,\n",
    "    'expand_vocab': True,\n",
    "}\n",
    "\n",
    "print(f'Training {MODEL_TYPE}...')\n",
    "\n",
    "# Train incrementally, important to use fixed test data\n",
    "result = trainer.train(\n",
    "    model_name=MODEL_TYPE, # Set in the config\n",
    "    data=(X_train_incremental, y_train_incremental, X_test_fixed, y_test_fixed),\n",
    "    config=incremental_config,\n",
    "    job_id=f'transformer_incremental_{timestamp}',\n",
    ")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShVnhyWiAZmj"
   },
   "source": [
    "### Run same tests again but after incremental train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgPuIh3DAYyr",
    "outputId": "634f8307-a9d8-4c32-90a0-c35c8e4990ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 16:09:17,002 - ml_pipeline.inference.predictor - INFO - Loaded model from /content/drive/MyDrive/Colab Notebooks/ml_pipeline/sentio_results/increment/transformer_transformer_incremental_20251208_155418.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "======================================================================\n",
      "\n",
      "Text: \"I feel so stressed and anxious about everything lately.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Stress                99.9% <<<\n",
      "      Normal                 0.1%\n",
      "      Depression             0.0%\n",
      "      Suicidal               0.0%\n",
      "\n",
      "Text: \"Life is great, I'm feeling happy and content.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                98.9% <<<\n",
      "      Stress                 0.7%\n",
      "      Suicidal               0.4%\n",
      "      Depression             0.1%\n",
      "\n",
      "Text: \"I don't want to live anymore, everything is hopeless.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Suicidal              89.0% <<<\n",
      "      Depression            10.8%\n",
      "      Normal                 0.1%\n",
      "      Stress                 0.1%\n",
      "\n",
      "Text: \"I can't stop crying and I feel so empty inside.\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                68.5% <<<\n",
      "      Depression            17.5%\n",
      "      Suicidal               9.7%\n",
      "      Stress                 4.4%\n",
      "\n",
      "Text: \"I am happy and stressed\"\n",
      "----------------------------------------------------------------------\n",
      "  >>> Normal                54.9% <<<\n",
      "      Stress                44.6%\n",
      "      Depression             0.3%\n",
      "      Suicidal               0.2%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"I feel so stressed and anxious about everything lately.\",\n",
    "    \"Life is great, I'm feeling happy and content.\",\n",
    "    \"I don't want to live anymore, everything is hopeless.\",\n",
    "    \"I can't stop crying and I feel so empty inside.\",\n",
    "    \"I am happy and stressed\"\n",
    "]\n",
    "\n",
    "# Load predictor\n",
    "predictor = Predictor(storage)\n",
    "predictor.load(result['model_path'])\n",
    "\n",
    "print('Sample predictions:')\n",
    "print('=' * 70)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Preprocess text first (same as training)\n",
    "    processed = preprocessor._preprocess_single_text(text)\n",
    "\n",
    "    # Get prediction\n",
    "    pred = predictor.predict(processed)\n",
    "\n",
    "    print(f'\\nText: \"{text}\"')\n",
    "    print('-' * 70)\n",
    "\n",
    "    # Sort probabilities descending\n",
    "    sorted_probs = sorted(pred['probabilities'].items(), key=lambda x: -x[1])\n",
    "\n",
    "    for i, (label, prob) in enumerate(sorted_probs):\n",
    "        if i == 0:\n",
    "            print(f'  >>> {label:20} {prob*100:5.1f}% <<<')\n",
    "        else:\n",
    "            print(f'      {label:20} {prob*100:5.1f}%')\n",
    "\n",
    "print('\\n' + '=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdj7NkpEbYJo",
    "outputId": "eca2f2b5-e74e-49bc-b4bd-fb5c2ec8774b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental model accuracy: 0.7998\n",
      "Incremental model F1: 0.7994\n"
     ]
    }
   ],
   "source": [
    "print(f\"Incremental model accuracy: {result['metrics']['accuracy']:.4f}\")\n",
    "print(f\"Incremental model F1: {result['metrics']['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAcAiAFubgIF",
    "outputId": "ff84b693-2070-45cd-9587-4dd207100dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model:        0.7841\n",
      "Incremental model: 0.7998\n",
      "Change:            +0.0156\n",
      "Model improved!\n"
     ]
    }
   ],
   "source": [
    "new_accuracy = result['metrics']['accuracy']\n",
    "\n",
    "if base_accuracy is not None:\n",
    "    print(f\"Base model:        {base_accuracy:.4f}\")\n",
    "    print(f\"Incremental model: {new_accuracy:.4f}\")\n",
    "    print(f\"Change:            {new_accuracy - base_accuracy:+.4f}\")\n",
    "\n",
    "    if new_accuracy > base_accuracy:\n",
    "        print(\"Model improved!\")\n",
    "    else:\n",
    "        print(f\"Model did not improve with adding {len(X_train_incremental)} rows of data\")\n",
    "else:\n",
    "    print(f\"Incremental model: {new_accuracy:.4f}\")\n",
    "    print(\"(Base accuracy not available - evaluate base model separately to compare)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t_tcV4MdUWZV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
