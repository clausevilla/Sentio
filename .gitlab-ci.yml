stages:
  - setup
  - test
  # - train   To be implemented when model training is ready

variables:
  PYTHON_VERSION: "3.11"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

image: python:${PYTHON_VERSION}-slim

# Cache pip packages between jobs
cache:
  key: ${CI_COMMIT_REF_SLUG}
  paths:
    - .cache/pip

before_script:
  - echo "Installing dependencies..."
  - python -m pip install --upgrade pip
  - pip install -r requirements.txt --no-cache-dir

setup:
  stage: setup
  script:
    - python manage.py check    # Scan project for config / model issues
    - python manage.py migrate
  tags:
    - docker

test:
  stage: test
  script:
    - python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('omw-1.4'); nltk.download('averaged_perceptron_tagger'); nltk.download('averaged_perceptron_tagger_eng')"
    - echo "Running data pipeline and authentication tests..."
    - python manage.py test apps.ml_admin.tests --verbosity=2
    - python manage.py test apps.accounts.tests --verbosity=2
  tags:
    - docker

# train:
#   stage: train
#   script:
#     - echo "Training ML model with processed data..."
#     - python manage.py import_dataset data/stressor-data-combined.csv --dataset_type train --subset 5000
#     - python ml_models/bert/run_model.py --use-database --subset 5000  # Run model for training
#
#     # TODO: 'Import_dataset'command should call each pipeline

#   tags:
#     - docker
#   artifacts:
#     paths:
#       - ml_models/bert/trained_model/  # Save trained model artifact
#     expire_in: 1 week
